import os
import asyncio
import threading
import pickle
import warnings
from pathlib import Path
from dotenv import load_dotenv

import streamlit as st
# import nest_asyncio # Not strictly needed with the simplified async worker

# LangChain and Gemini setup
from langchain_core.documents import Document as LCDocument
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool, Tool
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_models import ChatOllama # Placeholder for a standard chat model
from langchain_core.runnables import RunnablePassthrough
from langgraph.graph import StateGraph, END

# --- Environment Setup (Simplified from original) ---

warnings.filterwarnings("ignore")
load_dotenv()

# Configure Google Gemini
# Use os.getenv directly for api keys
GEMINI_MODEL_NAME = "gemini-2.5-pro"
EMBEDDING_MODEL_NAME = "gemini-embedding-001"

llm = ChatGoogleGenerativeAI(model=GEMINI_MODEL_NAME, temperature=0)
embed_model = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL_NAME)

# Directories (Chroma will use the STORAGE_DIR)
DATA_DIR = Path("DATAN")
STORAGE_DIR = Path("storage_lc") # Use a new dir for LangChain/Chroma
SUMMARY_DIR = Path("summaries_lc") # Use a new dir for summaries
DATA_DIR.mkdir(exist_ok=True)
STORAGE_DIR.mkdir(exist_ok=True)
SUMMARY_DIR.mkdir(exist_ok=True)

# ChromaDB persistence setup
CHROMA_PERSIST_DIR = str(STORAGE_DIR / "chroma_db")

# --- Async Worker (Simplified) ---

# This worker is still needed for running the synchronous init_agents in a thread
# to avoid blocking Streamlit, but the internal logic is simpler as LangChain
# objects are generally synchronous unless explicitly using the async API.
class SyncWorker(threading.Thread):
    def __init__(self, func, *args):
        super().__init__()
        self.func = func
        self.args = args
        self.result = None

    def run(self):
        self.result = self.func(*self.args)

# --- Document Processing and Single Service Agent Builder ---

TEXT_SPLITTER = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

def build_service_agent(doc: LCDocument, base: str):
    """
    Builds a LangChain/Chroma retrieval system and tool for a single UDS document.
    
    Args:
        doc: The LangChain Document object.
        base: The base name for the service.
    
    Returns:
        A LangChain Tool for the service, and the service summary string.
    """
    
    # 1. Chroma DB for Vector Storage (Vector Index equivalent)
    # We use a unique collection name for each service for isolation
    chroma_collection_name = f"chroma_{base}"
    
    # Check if the vector store already exists (Chroma persistence check)
    try:
        # Attempt to load the existing collection
        vectorstore = Chroma(
            collection_name=chroma_collection_name, 
            persist_directory=CHROMA_PERSIST_DIR, 
            embedding_function=embed_model
        )
        if not vectorstore._collection.count(): # If collection is empty, re-add documents
             vectorstore = Chroma.from_documents(
                [doc], 
                embed_model, 
                collection_name=chroma_collection_name, 
                persist_directory=CHROMA_PERSIST_DIR
            )
        vectorstore.persist()
    except Exception:
        # If loading fails (e.g., first run), create from documents
        vectorstore = Chroma.from_documents(
            [doc], 
            embed_model, 
            collection_name=chroma_collection_name, 
            persist_directory=CHROMA_PERSIST_DIR
        )
        vectorstore.persist()

    retriever = vectorstore.as_retriever()

    # 2. Retrieval Chain (Vector Query Engine equivalent)
    # The prompt for the retrieval chain
    vector_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are an expert on UDS service '{base}'. Use the provided context to answer the user's question. If you don't know the answer, just say you don't have enough information."),
        ("user", "{input}")
    ])
    
    document_chain = create_stuff_documents_chain(llm, vector_prompt.partial(base=base))
    vqe_chain = create_retrieval_chain(retriever, document_chain)

    # 3. Summary Generation (Summary Index equivalent)
    summary_out = SUMMARY_DIR / f"{base}.pkl"
    if not summary_out.exists():
        # Generate summary using the LLM directly on the full document text
        summary_prompt = f"Summarize this UDS service in 1‚Äì2 lines. The service is:\n\n{doc.page_content[:2000]}..." # Use first part of content
        s = llm.invoke(summary_prompt).content
        pickle.dump(str(s), open(summary_out, "wb"))
    
    summary = pickle.load(open(summary_out, "rb"))

    # 4. Create the LangChain Tool
    @tool(name=f"tool_{base}", description=f"Tool for semantic search within the {base} UDS service. {summary}")
    def service_query_tool(query: str) -> str:
        """Runs a semantic search query against the specific UDS service's document."""
        # Run the retrieval chain synchronously
        response = vqe_chain.invoke({"input": query})
        # The result is in 'answer' from the retrieval chain
        return response['answer']

    return service_query_tool, summary

# --- Main Agent Builder ---

def init_agents():
    """Initializes all single-service tools and the top-level ReAct/LangGraph agent."""
    
    # 1. Load documents
    docs = []
    for file in DATA_DIR.glob("*.txt"):
        text = file.read_text(encoding='utf-8')
        # LangChain Document
        docs.append(LCDocument(page_content=text, metadata={"path": file.stem}))

    # 2. Build Tools and Summaries for each service
    service_tools = []
    summaries = {}
    for doc in docs:
        base = doc.metadata["path"]
        # Split document into chunks before passing to tool builder for initial storage
        chunks = TEXT_SPLITTER.split_documents([doc])
        # We pass the full doc for summary and the first chunk for storage setup
        tool, summary = build_service_agent(LCDocument(page_content=doc.page_content, metadata=doc.metadata), base)
        service_tools.append(tool)
        summaries[base] = summary

    # 3. Baseline RAG setup
    # Combine all nodes (chunks) for a single baseline RAG index
    all_chunks = []
    for doc in docs:
        all_chunks.extend(TEXT_SPLITTER.split_documents([doc]))

    # Use a separate Chroma collection for the baseline RAG
    baseline_chroma_name = "chroma_baseline"
    baseline_vectorstore = Chroma.from_documents(
        all_chunks, 
        embed_model, 
        collection_name=baseline_chroma_name, 
        persist_directory=CHROMA_PERSIST_DIR
    )
    baseline_retriever = baseline_vectorstore.as_retriever(search_kwargs={"k": 4})
    
    # Baseline RAG Chain
    baseline_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a general UDS expert. Use the provided context to answer the user's question."),
        ("user", "{input}")
    ])
    baseline_document_chain = create_stuff_documents_chain(llm, baseline_prompt)
    base_qe = create_retrieval_chain(baseline_retriever, baseline_document_chain)


    # 4. Top-level Multi-Agent (ReAct Agent equivalent via standard LangChain AgentExecutor)
    
    # The custom retriever logic is *not* needed because LangChain's AgentExecutor 
    # handles tool selection using the LLM and the descriptions provided in the tools.
    # The LangChain ReAct agent *is* the top-level agent.
    
    top_agent_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a UDS expert. Your task is to use the provided tools to answer the user's question. Select the correct tool(s) based on the service name or comparison need. Use only tools. Be concise."),
        ("placeholder", "{agent_scratchpad}"),
        ("user", "{input}")
    ])

    top_agent = create_react_agent(llm, service_tools, top_agent_prompt)
    top_agent_executor = AgentExecutor(
        agent=top_agent, 
        tools=service_tools, 
        verbose=True, # Set to True for debugging tool usage
        handle_parsing_errors=True
    )

    return top_agent_executor, base_qe

# --- Streamlit UI ---

st.set_page_config(page_title="UDS Chatbot", layout="wide")
st.title("üöó UDS Service Chatbot (LangChain/Gemini/Chroma)")

# File uploader
uploaded_files = st.file_uploader("üìÅ Upload UDS service `.txt` files", type=["txt"], accept_multiple_files=True)
if uploaded_files:
    for uf in uploaded_files:
        out_path = DATA_DIR / uf.name
        with open(out_path, "wb") as f:
            f.write(uf.getvalue())
    st.success("‚úÖ Files uploaded! Click 'Initialize Services' to build agents.")

# Initialize button
if st.button("Initialize Services"):
    with st.spinner("üîß Building indices and agents..."):
        # Run initialization in a thread to prevent Streamlit from timing out
        worker = SyncWorker(init_agents)
        worker.start()
        worker.join() # Wait for the thread to complete
        
        # Get results
        top_agent, base_qe = worker.result
        
        st.session_state["top_agent"] = top_agent
        st.session_state["base_qe"] = base_qe
        st.session_state["history"] = []
    st.success("‚úÖ Initialization complete!")

# Chat Interface
if "top_agent" in st.session_state:

    # Async function wrapper for Streamlit
    async def query_agents(query):
        # LangChain AgentExecutor is synchronous by default, so we run it in a thread
        # The baseline RAG is also synchronous.
        
        def run_sync_queries():
            # Run Top Agent (Multi-agent reasoning)
            # The top_agent_executor is synchronous
            res1_obj = st.session_state["top_agent"].invoke({"input": query})
            res1 = res1_obj.get("output", "Error: No output from TopAgent.")
            
            # Run Baseline RAG
            res2_obj = st.session_state["base_qe"].invoke({"input": query})
            res2 = res2_obj.get("answer", "Error: No answer from Baseline RAG.")
            
            return res1, res2
            
        # Use an executor to run the synchronous calls in a background thread
        loop = asyncio.get_event_loop()
        res1, res2 = await loop.run_in_executor(None, run_sync_queries)
        
        return res1, res2

    # Chat input
    query = st.chat_input("üí¨ Ask about UDS services...")

    if query:
        st.session_state["history"].append(("user", query))
        
        # Need to use the event loop for the async function wrapper
        try:
            res1, res2 = asyncio.run(query_agents(query))
        except RuntimeError:
            # Fallback for environments where asyncio.run might fail
            loop = asyncio.new_event_loop()
            res1, res2 = loop.run_until_complete(query_agents(query))
        
        st.session_state["history"].append(("top_agent", res1))
        st.session_state["history"].append(("baseline", res2))

    # Display history
    for sender, msg in st.session_state["history"]:
        if sender == "user":
            with st.chat_message("user"):
                st.markdown(msg)
        elif sender == "top_agent":
            with st.chat_message("assistant"):
                st.markdown(f"üîπ **TopAgent (LangChain Multi-agent):**\n\n{msg}")
        elif sender == "baseline":
            with st.chat_message("assistant"):
                st.markdown(f"üî∏ **Baseline RAG:**\n\n{msg}")

else:
    st.info("‚¨ÜÔ∏è Please upload files and initialize services to start chatting.")
